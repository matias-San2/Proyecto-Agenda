# üè• Ingenier√≠a del Caos ‚Äì Proyecto Hospital Padre Hurtado

**Fecha:** 7 de noviembre de 2025  
**Entorno:** AWS Academy ‚Äì EC2 Ubuntu, Serverless Framework, DynamoDB Local/Remoto  

---

## üîç Introducci√≥n

El sistema **Hospital Padre Hurtado** es una plataforma integral de gesti√≥n hospitalaria desarrollada con **Node.js**, **Express**, y una arquitectura **serverless** desplegada en **AWS Lambda**.  
Su infraestructura combina servicios gestionados de **Cognito (autenticaci√≥n JWT)**, **API Gateway**, **DynamoDB**, y **MySQL**, junto con entornos de despliegue en **EC2** bajo integraci√≥n y entrega continua (**CI/CD con GitHub Actions**).

El sistema incluye:
- Autenticaci√≥n con Cognito y control granular de roles.
- M√≥dulos de gesti√≥n de consultas, agendas m√©dicas y notificaciones.
- Personalizaci√≥n din√°mica de interfaz (i18n espa√±ol/ingl√©s).
- Funciones Lambda para acceso y administraci√≥n de datos hospitalarios.
- Pipelines autom√°ticos de despliegue y seguridad (CodeQL, Dependabot, PM2).

Esta infraestructura, altamente distribuida y dependiente de microservicios, hace esencial aplicar **Ingenier√≠a del Caos** para validar su tolerancia a fallos, su capacidad de recuperaci√≥n y la integridad de su comunicaci√≥n entre componentes.

---

## üéØ Objetivo General

Evaluar la resiliencia operativa del sistema frente a fallas controladas, simulando interrupciones en los servicios cr√≠ticos (DynamoDB, Lambda, red) y midiendo la capacidad de detecci√≥n, respuesta y recuperaci√≥n del backend hospitalario.

---

## üß™ Experimento 1: Latencia y error controlado (`/chaos-test`)

**Objetivo:** Validar la resiliencia del sistema frente a fallas aleatorias y latencia inducida, comprobando si el backend contin√∫a operativo ante errores internos simulados.

**Procedimiento:**
- Se implement√≥ un endpoint `/chaos-test` que alterna entre estados *healthy*, *failed* y *unhealthy* con retardos aleatorios.
- Se realizaron 14 invocaciones consecutivas con `curl` para evaluar el comportamiento ante m√∫ltiples escenarios de error.

**Resultados:**

| Estado | Conteo | Porcentaje aproximado | Ejemplo |
|---------|---------|------------------------|----------|
| üü¢ healthy | 8 | 57% | `{ "status": "healthy", "dynamodb": "connected", "delay": "2104ms" }` |
| üî¥ failed | 3 | 21% | `{ "status": "failed", "error": "Simulated internal server error", "delay": "2364ms" }` |
| üü† unhealthy (DynamoDB desconectado) | 2 | 14% | `{ "status": "unhealthy", "dynamodb": "disconnected", "delay": "1933ms" }` |
| ‚ö™ Otros (reinicio/corte manual) | 1 | 8% | ‚Äî |

**Ejemplos de respuestas:**
```json
{"status":"failed","error":"Simulated internal server error","delay":"2364ms","timestamp":"2025-11-07T17:30:29.751Z"}
{"status":"healthy","dynamodb":"connected","delay":"2807ms","chaosMode":"active","timestamp":"2025-11-07T17:34:53.306Z"}
{"status":"unhealthy","dynamodb":"disconnected","delay":"1933ms","chaosMode":"active","timestamp":"2025-11-07T17:35:42.330Z"}
```

**An√°lisis:**
- La latencia promedio fue de **1.9 segundos** (rango: 943 ms ‚Äì 2.9 s).
- El sistema altern√≥ correctamente entre estados *healthy*, *failed* y *unhealthy* sin detener el servicio.
- Se observaron reconexiones autom√°ticas a DynamoDB tras las simulaciones de falla.
- El comportamiento ca√≥tico fue controlado, manteniendo estabilidad general y sin errores persistentes.

**Conclusi√≥n:**
> El experimento demostr√≥ la capacidad del backend para tolerar y recuperarse de fallas controladas en tiempo real. El sistema altern√≥ entre estados *healthy*, *failed* y *unhealthy* sin requerir reinicios manuales. La latencia media (~1.9s) se mantuvo dentro de m√°rgenes aceptables considerando la simulaci√≥n de caos.  
> **Conclusi√≥n final:** El sistema es resiliente ante interrupciones simuladas, mantiene conectividad parcial con DynamoDB y conserva la estabilidad general del backend.---

## üß™ Experimento 2: Falla simulada de DynamoDB (`/health`)

**Objetivo:** Evaluar la detecci√≥n y recuperaci√≥n del sistema ante una ca√≠da del servicio de base de datos.

**Procedimiento:**
- Simulaci√≥n de desconexi√≥n DynamoDB mediante c√≥digo.
- Posterior restauraci√≥n del servicio y verificaci√≥n.

**Resultados:**
```json
{"status":"unhealthy","timestamp":"2025-11-07T17:50:33.815Z","error":"Simulated DynamoDB failure","stage":"dev"}
```
‚Üí Luego:
```json
{"status":"healthy","timestamp":"2025-11-07T18:00:46.251Z","stage":"dev","version":"1.0.0","responseTime":"139ms"}
```

**An√°lisis:**
- El sistema entra correctamente en estado *unhealthy* (503) y retorna a *healthy* tras la reconexi√≥n.
- La recuperaci√≥n fue inmediata sin reinicios adicionales.

**Conclusi√≥n:**  
La API es capaz de identificar y notificar fallas cr√≠ticas en DynamoDB, recuper√°ndose sin intervenci√≥n manual.

---

## üß™ Experimento 3: Prueba de estabilidad y recuperaci√≥n continua (`/health`)

**Objetivo:** Verificar disponibilidad, latencia y recuperaci√≥n tras reinicios controlados.

**Procedimiento:**
- Script que consulta `/health` cada 2 segundos (20 ciclos).
- Interrupci√≥n manual del servicio (`Ctrl + C`) y reinicio posterior.
- Log generado: `health_logs.txt`.

**Resultados:**
- 17/20 respuestas exitosas (`healthy`).
- Latencia promedio: 15 ms (m√°x. 137 ms).
- 3 omisiones coinciden con reinicio manual.

**Ejemplo:**
```json
{"status":"healthy","timestamp":"2025-11-07T18:38:50.576Z","stage":"dev","responseTime":"10ms"}
```

**An√°lisis:**
- Alta eficiencia y r√°pida recuperaci√≥n tras interrupci√≥n.
- Microcortes no afectaron la integridad del sistema.

**Conclusi√≥n:**  
El servicio conserva su estabilidad operativa y se recupera autom√°ticamente tras reinicios o ca√≠das temporales.

---

## Conclusiones Globales

| Aspecto Evaluado | Resultado | Evidencia |
|------------------|------------|------------|
| Tolerancia a errores internos | ‚úÖ Exitosa | `/chaos-test` con error controlado |
| Recuperaci√≥n ante ca√≠da de BD | ‚úÖ Exitosa | `/health` ‚Üí unhealthy ‚Üí healthy |
| Estabilidad bajo monitoreo continuo | ‚úÖ Alta | `health_logs.txt` con 85% respuestas correctas |
| Tiempo promedio de respuesta | ‚ö° 15 ms | Consistente en 17/20 solicitudes |
| Recuperaci√≥n tras reinicio | ‚úÖ Autom√°tica | Sin p√©rdida de estado ni datos |

**Conclusi√≥n general:**  
El sistema **Hospital Padre Hurtado** cumple con los principios de *Chaos Engineering*: resistencia, detecci√≥n proactiva y recuperaci√≥n aut√≥noma.  
Se recomienda incorporar monitoreo permanente con **CloudWatch Metrics** y alarmas **SNS** para escalar notificaciones ante fallas reales en producci√≥n.

---

**Repositorio:**  
üîó [GitHub ‚Äì Proyecto Hospital Padre Hurtado](https://github.com/Incodefy/Proyecto-Hospital-Padre-Hurtado)  

